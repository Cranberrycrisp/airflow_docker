version: '3.3'
services:
  mysql:
    hostname: 'mysql'
    container_name: 'mysql'
    image: mysql/mysql-server:${MYSQL_VERSION}
    restart: always
    ports: 
      - 3306:3306
      - 33060:33060
    environment: 
      - MYSQL_DATABASE=${AIRFLOW_DATABASE}
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
      - MYSQL_ROOT_HOST=% 
  
  broker:
    hostname: 'broker'
    container_name: 'rabbitmq'
    image: rabbitmq:${RABBITMQ_VERSION}
    restart: always
    ports: 
      - 5672:5672
  
  celery:
    hostname: 'celery'
    container_name: 'celery'
    build:
      context: .
      dockerfile: Dockerfile
    image: celery:5
    depends_on: 
      - broker
    restart: always
    ports: 
      - 5555:5555
    environment: 
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CELERY__CELERY_APP_NAME=${AIRFLOW__CELERY__CELERY_APP_NAME}
      - AIRFLOW__CELERY__BROKER_URL=${AIRFLOW__CELERY__BROKER_URL}
      - AIRFLOW__CELERY__WORKER_CONCURRENCY=${AIRFLOW__CELERY__WORKER_CONCURRENCY}
      - AIRFLOW__CELERY__RESULT_BACKEND=${AIRFLOW__CELERY__RESULT_BACKEND}
    command: bash -c "airflow celery flower"
  
  airflow:
    hostname: 'airflow'
    container_name: 'airflow'
    build:
      context: .
      dockerfile: Dockerfile
    image: airflow:2.0.1
    depends_on: 
      - mysql
      - broker
    ports:
      - "8080:8080"
    restart: always
    environment: 
      - AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW__CORE__DAGS_FOLDER}
      - AIRFLOW__CORE__PLUGINS_FOLDER=${AIRFLOW__CORE__PLUGINS_FOLDER}
      - AIRFLOW__LOGGING__BASE_LOG_FOLDER=${AIRFLOW__LOGGING__BASE_LOG_FOLDER}
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
    volumes: 
      - dags:${AIRFLOW__CORE__DAGS_FOLDER}
    entrypoint: ['bash', 'entrypoint.sh']
  
  scheduler:
    hostname: 'scheduler'
    container_name: 'scheduler'
    build: 
      context: .
      dockerfile: Dockerfile
    image: schedular
    depends_on: 
      - airflow
    restart: always
    environment: 
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
    command: bash -c "airflow scheduler"

  worker_1:
    hostname: 'worker_1'
    container_name: 'worker_1'
    build:
      context: .
      dockerfile: Dockerfile
    image: worker
    depends_on: 
      - scheduler
    restart: always
    environment: 
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CELERY__CELERY_APP_NAME=${AIRFLOW__CELERY__CELERY_APP_NAME}
      - AIRFLOW__CELERY__BROKER_URL=${AIRFLOW__CELERY__BROKER_URL}
      - AIRFLOW__CELERY__WORKER_CONCURRENCY=${AIRFLOW__CELERY__WORKER_CONCURRENCY}
      - AIRFLOW__CELERY__RESULT_BACKEND=${AIRFLOW__CELERY__RESULT_BACKEND}
    volumes: 
      - dags:${AIRFLOW__CORE__DAGS_FOLDER}
    command: bash -c "airflow celery worker"
  
  worker_2:
    hostname: 'worker_2'
    container_name: 'worker_2'
    build:
      context: .
      dockerfile: Dockerfile
    image: worker
    depends_on: 
      - scheduler
    restart: always
    environment: 
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CELERY__CELERY_APP_NAME=${AIRFLOW__CELERY__CELERY_APP_NAME}
      - AIRFLOW__CELERY__BROKER_URL=${AIRFLOW__CELERY__BROKER_URL}
      - AIRFLOW__CELERY__WORKER_CONCURRENCY=${AIRFLOW__CELERY__WORKER_CONCURRENCY}
      - AIRFLOW__CELERY__RESULT_BACKEND=${AIRFLOW__CELERY__RESULT_BACKEND}
    volumes: 
      - dags:${AIRFLOW__CORE__DAGS_FOLDER}
    command: bash -c "airflow celery worker"

volumes: 
  dags: